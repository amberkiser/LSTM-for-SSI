{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd462226",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a9eb6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('../../data/train_long_data.pkl')\n",
    "validation_data = pd.read_pickle('../../data/val_long_data.pkl')\n",
    "test_data = pd.read_pickle('../../data/test_long_data.pkl')\n",
    "outcomes = pd.read_pickle('../../data/SSI_outcomes.pkl')\n",
    "\n",
    "with open('../../data/feature_selection_50_columns.pkl', 'rb') as f:\n",
    "    keep_columns = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb7c53",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Prep for RNN\n",
    "\n",
    "## Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad50ba",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate time steps by days\n",
    "def isolate_categorical_features(data, time_agg = 'DAYS'):\n",
    "    categorical = data.loc[data['TERMINOLOGY'] != 'LOINC'].copy()\n",
    "\n",
    "    count = categorical[['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'FEATURE','VALUE']].groupby(['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'FEATURE']).sum().reset_index()\n",
    "    count = count.pivot_table(values='VALUE', index=['PT_KEY', 'TIME_AFTER_OP_%s' % time_agg], columns='FEATURE').reset_index()\n",
    "    return count\n",
    "\n",
    "def isolate_numeric_features(data, time_agg = 'DAYS'):\n",
    "    numerical = data.loc[data['TERMINOLOGY'] == 'LOINC'].copy()\n",
    "\n",
    "    median = numerical[['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'FEATURE','VALUE']].groupby(['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'FEATURE']).median().reset_index()\n",
    "    median['FEATURE'] = median['FEATURE'] + '_MEDIAN'\n",
    "    median = median.pivot_table(values='VALUE', index=['PT_KEY','TIME_AFTER_OP_%s' % time_agg], columns='FEATURE').reset_index()\n",
    "\n",
    "    mean = numerical[['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'FEATURE','VALUE']].groupby(['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'FEATURE']).mean().reset_index()\n",
    "    mean['FEATURE'] = mean['FEATURE'] + '_MEAN'\n",
    "    mean = mean.pivot_table(values='VALUE', index=['PT_KEY','TIME_AFTER_OP_%s' % time_agg], columns='FEATURE').reset_index()\n",
    "\n",
    "    minimum = numerical[['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'FEATURE','VALUE']].groupby(['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'FEATURE']).min().reset_index()\n",
    "    minimum['FEATURE'] = minimum['FEATURE'] + '_MIN'\n",
    "    minimum = minimum.pivot_table(values='VALUE', index=['PT_KEY','TIME_AFTER_OP_%s' % time_agg], columns='FEATURE').reset_index()\n",
    "\n",
    "    maximum = numerical[['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'FEATURE','VALUE']].groupby(['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'FEATURE']).max().reset_index()\n",
    "    maximum['FEATURE'] = maximum['FEATURE'] + '_MAX'\n",
    "    maximum = maximum.pivot_table(values='VALUE', index=['PT_KEY','TIME_AFTER_OP_%s' % time_agg], columns='FEATURE').reset_index()\n",
    "\n",
    "    return median, mean, minimum, maximum\n",
    "\n",
    "def create_agg_by_time_data(data, selected_columns, time_agg = 'DAYS'):\n",
    "    count = isolate_categorical_features(data, time_agg)\n",
    "    median, mean, minimum, maximum = isolate_numeric_features(data, time_agg)\n",
    "    \n",
    "    agg_data = data[['PT_KEY','TIME_AFTER_OP_%s' % time_agg]].drop_duplicates().merge(count, how='left', on=['PT_KEY','TIME_AFTER_OP_%s' % time_agg])\n",
    "    agg_data = agg_data.merge(median, how='left', on=['PT_KEY','TIME_AFTER_OP_%s' % time_agg])\n",
    "    agg_data = agg_data.merge(mean, how='left', on=['PT_KEY','TIME_AFTER_OP_%s' % time_agg])\n",
    "    agg_data = agg_data.merge(minimum, how='left', on=['PT_KEY','TIME_AFTER_OP_%s' % time_agg])\n",
    "    agg_data = agg_data.merge(maximum, how='left', on=['PT_KEY','TIME_AFTER_OP_%s' % time_agg])\n",
    "    \n",
    "    agg_data = pd.concat([agg_data[['PT_KEY','TIME_AFTER_OP_%s' % time_agg]], agg_data[selected_columns]], axis=1)\n",
    "    \n",
    "    return agg_data\n",
    "\n",
    "# Fill NAs\n",
    "def fill_NAs(data, medians):\n",
    "    data = data.fillna(medians.to_dict())\n",
    "    data = data.fillna(0)\n",
    "    return data\n",
    "\n",
    "# Scale data\n",
    "def get_scaled_data(data, scaler, time_agg):\n",
    "    scaled_data = pd.DataFrame(scaler.transform(data.drop(columns=['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'SSI'])), \n",
    "                               columns = [col for col in data.columns if col not in ['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'SSI']])\n",
    "    scaled_data = pd.concat([data[['PT_KEY','TIME_AFTER_OP_%s' % time_agg,'SSI']], scaled_data], axis=1)\n",
    "    return scaled_data\n",
    "\n",
    "# Transform to 3D array\n",
    "def make_3D_array(data, time_agg, max_value):\n",
    "    pt_list = data['PT_KEY'].drop_duplicates().values\n",
    "\n",
    "    array_list = []\n",
    "    for pt_key in pt_list:\n",
    "        temp_array = data.loc[data['PT_KEY'] == pt_key].copy().drop(columns=['PT_KEY','SSI']).values\n",
    "        if temp_array.shape[0] > max_value:\n",
    "            temp_array = temp_array[:max_value]\n",
    "        else:\n",
    "            temp_array = np.pad(temp_array, [(max_value-temp_array.shape[0], 0), (0,0)], 'constant', constant_values=-1)\n",
    "        array_list.append(temp_array)\n",
    "    return np.array(array_list)\n",
    "    \n",
    "# Get outcome data as numpy array\n",
    "def get_label_array(data, max_value):\n",
    "    label_data = data[['PT_KEY','SSI']].drop_duplicates()\n",
    "    \n",
    "    array_list = []\n",
    "    for label in label_data['SSI'].values:\n",
    "        array_list.append(np.repeat(label, max_value))\n",
    "    labels = np.array(array_list)\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287989d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train Data Prep\n",
    "N = 6430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0f1e7e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trn_data = create_agg_by_time_data(train_data, keep_columns, 'DAYS')\n",
    "# trn_data = create_agg_by_time_data(train_data, keep_columns, 'HR')\n",
    "medians = trn_data[[col for col in trn_data.columns if re.search('LOINC', col)]].median()\n",
    "trn_data = fill_NAs(trn_data, medians)\n",
    "trn_data = trn_data.merge(outcomes, how='left', on='PT_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1c829",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Scale Data\n",
    "Use MinMaxScaler (normalize values between 0 and 1) since data is not necessarily normal \n",
    "\n",
    "Save scaler from training data to apply to testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac27bd94",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(trn_data.drop(columns=['PT_KEY','TIME_AFTER_OP_DAYS','SSI']))\n",
    "# scaler.fit(trn_data.drop(columns=['PT_KEY','TIME_AFTER_OP_HR','SSI']))\n",
    "\n",
    "trn_data = get_scaled_data(trn_data, scaler, 'DAYS')\n",
    "# trn_data = get_scaled_data(trn_data, scaler, 'HR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8a026b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split training data into 10 stratified folds\n",
    "\n",
    "Used for cross validation in tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dbd65a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cv_data = trn_data[['PT_KEY','SSI']].drop_duplicates().reset_index(drop=True)\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "fold_ids = {'train':{}, 'val':{}}\n",
    "fold = 0\n",
    "for train_index, val_index in cv.split(cv_data['PT_KEY'], cv_data['SSI']):\n",
    "    fold += 1\n",
    "    fold_train_data = cv_data.iloc[train_index]\n",
    "    fold_ids['train'][fold] = fold_train_data['PT_KEY'].values\n",
    "    fold_val_data = cv_data.iloc[val_index]\n",
    "    fold_ids['val'][fold] = fold_val_data['PT_KEY'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0d73c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trn_fold_1 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['train'][1])].copy()\n",
    "trn_fold_2 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['train'][2])].copy()\n",
    "trn_fold_3 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['train'][3])].copy()\n",
    "trn_fold_4 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['train'][4])].copy()\n",
    "trn_fold_5 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['train'][5])].copy()\n",
    "trn_fold_6 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['train'][6])].copy()\n",
    "trn_fold_7 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['train'][7])].copy()\n",
    "trn_fold_8 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['train'][8])].copy()\n",
    "trn_fold_9 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['train'][9])].copy()\n",
    "trn_fold_10 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['train'][10])].copy()\n",
    "\n",
    "val_fold_1 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['val'][1])].copy()\n",
    "val_fold_2 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['val'][2])].copy()\n",
    "val_fold_3 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['val'][3])].copy()\n",
    "val_fold_4 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['val'][4])].copy()\n",
    "val_fold_5 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['val'][5])].copy()\n",
    "val_fold_6 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['val'][6])].copy()\n",
    "val_fold_7 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['val'][7])].copy()\n",
    "val_fold_8 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['val'][8])].copy()\n",
    "val_fold_9 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['val'][9])].copy()\n",
    "val_fold_10 = trn_data.loc[trn_data['PT_KEY'].isin(fold_ids['val'][10])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f44ee6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transform into 3D numpy array\n",
    "\n",
    "Final step would be to coerce into 3D array: each operative event on y-axis, each feature on x-axis, and each time on z-axis\n",
    "\n",
    "Operative events do not all have the same number of days, need to pad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286d2aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "max_value = trn_data[['PT_KEY','TIME_AFTER_OP_DAYS']].groupby('PT_KEY').count().max().values[0]\n",
    "# max_value = trn_data[['PT_KEY','TIME_AFTER_OP_HR']].groupby('PT_KEY').count().max().values[0]\n",
    "\n",
    "train_X = make_3D_array(trn_data, 'DAYS', max_value)\n",
    "# train_X = make_3D_array(trn_data, 'HR', max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43af10",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_X_1 = make_3D_array(trn_fold_1, 'DAYS', max_value)\n",
    "train_X_2 = make_3D_array(trn_fold_2, 'DAYS', max_value)\n",
    "train_X_3 = make_3D_array(trn_fold_3, 'DAYS', max_value)\n",
    "train_X_4 = make_3D_array(trn_fold_4, 'DAYS', max_value)\n",
    "train_X_5 = make_3D_array(trn_fold_5, 'DAYS', max_value)\n",
    "train_X_6 = make_3D_array(trn_fold_6, 'DAYS', max_value)\n",
    "train_X_7 = make_3D_array(trn_fold_7, 'DAYS', max_value)\n",
    "train_X_8 = make_3D_array(trn_fold_8, 'DAYS', max_value)\n",
    "train_X_9 = make_3D_array(trn_fold_9, 'DAYS', max_value)\n",
    "train_X_10 = make_3D_array(trn_fold_10, 'DAYS', max_value)\n",
    "\n",
    "val_X_1 = make_3D_array(val_fold_1, 'DAYS', max_value)\n",
    "val_X_2 = make_3D_array(val_fold_2, 'DAYS', max_value)\n",
    "val_X_3 = make_3D_array(val_fold_3, 'DAYS', max_value)\n",
    "val_X_4 = make_3D_array(val_fold_4, 'DAYS', max_value)\n",
    "val_X_5 = make_3D_array(val_fold_5, 'DAYS', max_value)\n",
    "val_X_6 = make_3D_array(val_fold_6, 'DAYS', max_value)\n",
    "val_X_7 = make_3D_array(val_fold_7, 'DAYS', max_value)\n",
    "val_X_8 = make_3D_array(val_fold_8, 'DAYS', max_value)\n",
    "val_X_9 = make_3D_array(val_fold_9, 'DAYS', max_value)\n",
    "val_X_10 = make_3D_array(val_fold_10, 'DAYS', max_value)\n",
    "\n",
    "# train_X_1 = make_3D_array(trn_fold_1, 'HR', max_value)\n",
    "# train_X_2 = make_3D_array(trn_fold_2, 'HR', max_value)\n",
    "# train_X_3 = make_3D_array(trn_fold_3, 'HR', max_value)\n",
    "# train_X_4 = make_3D_array(trn_fold_4, 'HR', max_value)\n",
    "# train_X_5 = make_3D_array(trn_fold_5, 'HR', max_value)\n",
    "# train_X_6 = make_3D_array(trn_fold_6, 'HR', max_value)\n",
    "# train_X_7 = make_3D_array(trn_fold_7, 'HR', max_value)\n",
    "# train_X_8 = make_3D_array(trn_fold_8, 'HR', max_value)\n",
    "# train_X_9 = make_3D_array(trn_fold_9, 'HR', max_value)\n",
    "# train_X_10 = make_3D_array(trn_fold_10, 'HR', max_value)\n",
    "\n",
    "# val_X_1 = make_3D_array(val_fold_1, 'HR', max_value)\n",
    "# val_X_2 = make_3D_array(val_fold_2, 'HR', max_value)\n",
    "# val_X_3 = make_3D_array(val_fold_3, 'HR', max_value)\n",
    "# val_X_4 = make_3D_array(val_fold_4, 'HR', max_value)\n",
    "# val_X_5 = make_3D_array(val_fold_5, 'HR', max_value)\n",
    "# val_X_6 = make_3D_array(val_fold_6, 'HR', max_value)\n",
    "# val_X_7 = make_3D_array(val_fold_7, 'HR', max_value)\n",
    "# val_X_8 = make_3D_array(val_fold_8, 'HR', max_value)\n",
    "# val_X_9 = make_3D_array(val_fold_9, 'HR', max_value)\n",
    "# val_X_10 = make_3D_array(val_fold_10, 'HR', max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057904ed",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Label Data\n",
    "\n",
    "Getting label data for both many to one and many to many models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d771302",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_y = get_label_array(trn_data, max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454a6dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_y_1 = get_label_array(trn_fold_1, max_value)\n",
    "train_y_2 = get_label_array(trn_fold_2, max_value)\n",
    "train_y_3 = get_label_array(trn_fold_3, max_value)\n",
    "train_y_4 = get_label_array(trn_fold_4, max_value)\n",
    "train_y_5 = get_label_array(trn_fold_5, max_value)\n",
    "train_y_6 = get_label_array(trn_fold_6, max_value)\n",
    "train_y_7 = get_label_array(trn_fold_7, max_value)\n",
    "train_y_8 = get_label_array(trn_fold_8, max_value)\n",
    "train_y_9 = get_label_array(trn_fold_9, max_value)\n",
    "train_y_10 = get_label_array(trn_fold_10, max_value)\n",
    "\n",
    "val_y_1 = get_label_array(val_fold_1, max_value)\n",
    "val_y_2 = get_label_array(val_fold_2, max_value)\n",
    "val_y_3 = get_label_array(val_fold_3, max_value)\n",
    "val_y_4 = get_label_array(val_fold_4, max_value)\n",
    "val_y_5 = get_label_array(val_fold_5, max_value)\n",
    "val_y_6 = get_label_array(val_fold_6, max_value)\n",
    "val_y_7 = get_label_array(val_fold_7, max_value)\n",
    "val_y_8 = get_label_array(val_fold_8, max_value)\n",
    "val_y_9 = get_label_array(val_fold_9, max_value)\n",
    "val_y_10 = get_label_array(val_fold_10, max_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481cb59",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Save Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d6af07",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Whole Training Set')\n",
    "print('Features: %s' %str(train_X.shape))\n",
    "print('Labels: %s' %str(train_y.shape))\n",
    "\n",
    "# medians.to_pickle('../../data/medians.pkl')\n",
    "# with open('../../data/scaler.pkl', 'wb') as f:\n",
    "#     pickle.dump(scaler, f)\n",
    "# trn_data.to_pickle('../../data/train_scaled.pkl')\n",
    "\n",
    "# with open('../../data/train_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X, f)\n",
    "# with open('../../data/train_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351f55f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Cross Validation Folds:')\n",
    "print('Fold 1')\n",
    "print('Train Features: %s' %str(train_X_1.shape))\n",
    "print('Train Labels: %s' %str(train_y_1.shape))\n",
    "print('Validation Features: %s' %str(val_X_1.shape))\n",
    "print('Validation Labels: %s' %str(val_y_1.shape))\n",
    "\n",
    "print('Fold 2')\n",
    "print('Train Features: %s' %str(train_X_2.shape))\n",
    "print('Train Labels: %s' %str(train_y_2.shape))\n",
    "print('Validation Features: %s' %str(val_X_2.shape))\n",
    "print('Validation Labels: %s' %str(val_y_2.shape))\n",
    "\n",
    "print('Fold 3')\n",
    "print('Train Features: %s' %str(train_X_3.shape))\n",
    "print('Train Labels: %s' %str(train_y_3.shape))\n",
    "print('Validation Features: %s' %str(val_X_3.shape))\n",
    "print('Validation Labels: %s' %str(val_y_3.shape))\n",
    "\n",
    "print('Fold 4')\n",
    "print('Train Features: %s' %str(train_X_4.shape))\n",
    "print('Train Labels: %s' %str(train_y_4.shape))\n",
    "print('Validation Features: %s' %str(val_X_4.shape))\n",
    "print('Validation Labels: %s' %str(val_y_4.shape))\n",
    "\n",
    "print('Fold 5')\n",
    "print('Train Features: %s' %str(train_X_5.shape))\n",
    "print('Train Labels: %s' %str(train_y_5.shape))\n",
    "print('Validation Features: %s' %str(val_X_5.shape))\n",
    "print('Validation Labels: %s' %str(val_y_5.shape))\n",
    "\n",
    "print('Fold 6')\n",
    "print('Train Features: %s' %str(train_X_6.shape))\n",
    "print('Train Labels: %s' %str(train_y.shape))\n",
    "print('Validation Features: %s' %str(val_X_6.shape))\n",
    "print('Validation Labels: %s' %str(val_y_6.shape))\n",
    "\n",
    "print('Fold 7')\n",
    "print('Train Features: %s' %str(train_X_7.shape))\n",
    "print('Train Labels: %s' %str(train_y_7.shape))\n",
    "print('Validation Features: %s' %str(val_X_7.shape))\n",
    "print('Validation Labels: %s' %str(val_y_7.shape))\n",
    "\n",
    "print('Fold 8')\n",
    "print('Train Features: %s' %str(train_X_8.shape))\n",
    "print('Train Labels: %s' %str(train_y_8.shape))\n",
    "print('Validation Features: %s' %str(val_X_8.shape))\n",
    "print('Validation Labels: %s' %str(val_y_8.shape))\n",
    "\n",
    "print('Fold 9')\n",
    "print('Train Features: %s' %str(train_X_9.shape))\n",
    "print('Train Labels: %s' %str(train_y_9.shape))\n",
    "print('Validation Features: %s' %str(val_X_9.shape))\n",
    "print('Validation Labels: %s' %str(val_y_9.shape))\n",
    "\n",
    "print('Fold 10')\n",
    "print('Train Features: %s' %str(train_X_10.shape))\n",
    "print('Train Labels: %s' %str(train_y_10.shape))\n",
    "print('Validation Features: %s' %str(val_X_10.shape))\n",
    "print('Validation Labels: %s' %str(val_y_10.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4905ff6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# with open('../../data/cv_data/train_1_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X_1, f)\n",
    "# with open('../../data/cv_data/train_1_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y_1, f)\n",
    "    \n",
    "# with open('../../data/cv_data/train_2_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X_2, f)\n",
    "# with open('../../data/cv_data/train_2_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y_2, f)\n",
    "    \n",
    "# with open('../../data/cv_data/train_3_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X_3, f)\n",
    "# with open('../../data/cv_data/train_3_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y_3, f)\n",
    "    \n",
    "# with open('../../data/cv_data/train_4_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X_4, f)\n",
    "# with open('../../data/cv_data/train_4_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y_4, f)\n",
    "    \n",
    "# with open('../../data/cv_data/train_5_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X_5, f)\n",
    "# with open('../../data/cv_data/train_5_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y_5, f)\n",
    "    \n",
    "# with open('../../data/cv_data/train_6_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X_6, f)\n",
    "# with open('../../data/cv_data/train_6_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y_6, f)\n",
    "    \n",
    "# with open('../../data/cv_data/train_7_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X_7, f)\n",
    "# with open('../../data/cv_data/train_7_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y_7, f)\n",
    "    \n",
    "# with open('../../data/cv_data/train_8_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X_8, f)\n",
    "# with open('../../data/cv_data/train_8_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y_8, f)\n",
    "    \n",
    "# with open('../../data/cv_data/train_9_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X_9, f)\n",
    "# with open('../../data/cv_data/train_9_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y_9, f)\n",
    "    \n",
    "# with open('../../data/cv_data/train_10_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_X_10, f)\n",
    "# with open('../../data/cv_data/train_10_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(train_y_10, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af580ce9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# with open('../../data/cv_data/val_1_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_X_1, f)\n",
    "# with open('../../data/cv_data/val_1_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_y_1, f)\n",
    "    \n",
    "# with open('../../data/cv_data/val_2_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_X_2, f)\n",
    "# with open('../../data/cv_data/val_2_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_y_2, f)\n",
    "    \n",
    "# with open('../../data/cv_data/val_3_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_X_3, f)\n",
    "# with open('../../data/cv_data/val_3_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_y_3, f)\n",
    "    \n",
    "# with open('../../data/cv_data/val_4_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_X_4, f)\n",
    "# with open('../../data/cv_data/val_4_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_y_4, f)\n",
    "    \n",
    "# with open('../../data/cv_data/val_5_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_X_5, f)\n",
    "# with open('../../data/cv_data/val_5_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_y_5, f)\n",
    "    \n",
    "# with open('../../data/cv_data/val_6_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_X_6, f)\n",
    "# with open('../../data/cv_data/val_6_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_y_6, f)\n",
    "    \n",
    "# with open('../../data/cv_data/val_7_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_X_7, f)\n",
    "# with open('../../data/cv_data/val_7_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_y_7, f)\n",
    "    \n",
    "# with open('../../data/cv_data/val_8_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_X_8, f)\n",
    "# with open('../../data/cv_data/val_8_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_y_8, f)\n",
    "    \n",
    "# with open('../../data/cv_data/val_9_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_X_9, f)\n",
    "# with open('../../data/cv_data/val_9_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_y_9, f)\n",
    "    \n",
    "# with open('../../data/cv_data/val_10_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_X_10, f)\n",
    "# with open('../../data/cv_data/val_10_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(val_y_10, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc3509b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Validation Data Prep \n",
    "N = 918\n",
    "\n",
    "Use medians, scaler, and max_value from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c85d4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_data = create_agg_by_time_data(validation_data, keep_columns, 'DAYS')\n",
    "# val_data = create_agg_by_time_data(validation_data, keep_columns, 'HR')\n",
    "val_data = fill_NAs(val_data, medians)\n",
    "val_data = val_data.merge(outcomes, how='left', on='PT_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8b0de",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db637ede",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_data = get_scaled_data(val_data, scaler, 'DAYS')\n",
    "# val_data = get_scaled_data(val_data, scaler, 'HR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2df691",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transform into 3D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13cc580",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_X = make_3D_array(val_data, 'DAYS', max_value)\n",
    "# val_X = make_3D_array(val_data, 'HR', max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58844611",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e9f4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_y = get_label_array(val_data, max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd04a1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acde3ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Validation Set')\n",
    "print('Features: %s' %str(val_X.shape))\n",
    "print('Labels: %s' %str(val_y.shape))\n",
    "\n",
    "val_data.to_pickle('../../data/val_scaled.pkl')\n",
    "\n",
    "with open('../../data/val_X.pkl', 'wb') as f:\n",
    "    pickle.dump(val_X, f)\n",
    "with open('../../data/val_y.pkl', 'wb') as f:\n",
    "    pickle.dump(val_y_m2o, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3fc970",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test Data Prep \n",
    "N = 1837\n",
    "\n",
    "Use medians, scaler, and max_value from train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27780bd8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tst_data = create_agg_by_time_data(test_data, keep_columns, 'DAYS')\n",
    "# tst_data = create_agg_by_time_data(test_data, keep_columns, 'HR')\n",
    "tst_data = fill_NAs(tst_data, medians)\n",
    "tst_data = tst_data.merge(outcomes, how='left', on='PT_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf401bb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8e8ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tst_data = get_scaled_data(tst_data, scaler, 'DAYS')\n",
    "# tst_data = get_scaled_data(tst_data, scaler, 'HR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ea880",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transform into 3D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811e6334",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_X = make_3D_array(tst_data, 'DAYS', max_value)\n",
    "# test_X = make_3D_array(tst_data, 'HR', max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d035eec",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c423316",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_y = get_label_array(tst_data, max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b3587",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Id Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270a554",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id_data = tst_data['PT_KEY'].drop_duplicates().values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ae8576",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Save Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c648dd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('Testing Set')\n",
    "print('Features: %s' %str(test_X.shape))\n",
    "print('Labels: %s' %str(test_y.shape))\n",
    "\n",
    "# tst_data.to_pickle('../../data/test_scaled.pkl')\n",
    "\n",
    "# with open('../../data/test_X.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_X, f)\n",
    "# with open('../../data/test_y.pkl', 'wb') as f:\n",
    "#     pickle.dump(test_y, f)\n",
    "# with open('../../data/test_ids.pkl', 'wb') as f:\n",
    "#     pickle.dump(id_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0415e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}